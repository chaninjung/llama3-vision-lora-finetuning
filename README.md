# ğŸ“š í”„ë¡œì íŠ¸ ê°œìš” / Project Overview

## í•œê¸€ ì„¤ëª…
ì´ ë¦¬í¬ì§€í† ë¦¬ëŠ” Meta-Llama 3.2 11B Vision-Instruct ëª¨ë¸ì„ 8-bit ì–‘ìí™” í›„ LoRA(PEFT) ë°©ì‹ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.  
A100 80GB í•œ ì¥ì—ì„œë„ ëŒ€ìš©ëŸ‰ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## English Explanation
This repository demonstrates how to fine-tune the Meta-Llama 3.2 11B Vision-Instruct model using 8-bit quantization and LoRA (PEFT).  
It is designed to run efficiently even on a single A100 80GB GPU.

---

# ğŸ“‚ íŒŒì¼ êµ¬ì¡° / File Structure

## í•œê¸€ ì„¤ëª…
```
llama3-vision-lora-finetuning/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_lora_finetune.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ placeholder.txt
â””â”€â”€ README_IMAGES/
```
- `.gitignore`: ë¶ˆí•„ìš”í•œ ìºì‹œ ë° ì²´í¬í¬ì¸íŠ¸ ì œì™¸  
- `LICENSE`: í”„ë¡œì íŠ¸ ë¼ì´ì„ ìŠ¤ ì •ë³´  
- `README.md`: ë³¸ ë¬¸ì„œ  
- `requirements.txt`: í•„ìš”í•œ íŒ¨í‚¤ì§€ ëª©ë¡  
- `scripts/`: í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ëª¨ìŒ  
- `data/`: ì‹¤ì œ ë°ì´í„°ëŠ” í¬í•¨ë˜ì§€ ì•Šìœ¼ë©°, placeholder.txt ì•ˆë‚´ë§Œ í¬í•¨  
- `README_IMAGES/`: (ì„ íƒ) ë‹¤ì´ì–´ê·¸ë¨ì´ë‚˜ ìŠ¤í¬ë¦°ìƒ· ë“±ì„ ì €ì¥  

## English Explanation
```
llama3-vision-lora-finetuning/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_lora_finetune.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ placeholder.txt
â””â”€â”€ README_IMAGES/
```
- `.gitignore`: Exclude unnecessary cache files and checkpoint folders  
- `LICENSE`: Project license information  
- `README.md`: This file  
- `requirements.txt`: List of required Python packages  
- `scripts/`: Fine-tuning scripts  
- `data/`: Placeholder only; actual JSONL data not included  
- `README_IMAGES/`: (Optional) Diagrams or screenshots  

---

# âš™ï¸ ì‚¬ì „ ì¤€ë¹„ / Prerequisites

## í•œê¸€ ì„¤ëª…
1. Python 3.9 ì´ìƒ  
2. A100 80GB GPU (ë˜ëŠ” ì¶©ë¶„í•œ VRAMì„ ê°–ì¶˜ GPU)  
3. ë¦¬í¬ì§€í† ë¦¬ í´ë¡  ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜  
   ```bash
   git clone https://github.com/YourUser/llama3-vision-lora-finetuning.git
   cd llama3-vision-lora-finetuning
   python3 -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```  
4. (ì„ íƒ) HuggingFace ë¡œê·¸ì¸  
   ```bash
   huggingface-cli login
   ```  
   ëª¨ë¸ ë‹¤ìš´ë¡œë“œìš© í† í°ì„ ë¯¸ë¦¬ ì„¤ì •í•´ ë‘ë©´ ì›í™œí•©ë‹ˆë‹¤.

## English Explanation
1. Python 3.9+  
2. A single A100 80GB GPU (or any GPU with enough VRAM for 8-bit + LoRA)  
3. Clone repo & install dependencies  
   ```bash
   git clone https://github.com/YourUser/llama3-vision-lora-finetuning.git
   cd llama3-vision-lora-finetuning
   python3 -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```  
4. (Optional) HuggingFace login  
   ```bash
   huggingface-cli login
   ```  
   Pre-configure your token to avoid download issues.

---

# ğŸš€ ì‚¬ìš© ë°©ë²• / How to Run

## í•œê¸€ ì„¤ëª…
1. `data/` ë””ë ‰í„°ë¦¬ì— JSONL í˜•íƒœì˜ íŒŒì¸íŠœë‹ ë°ì´í„°ë¥¼ ë„£ìŠµë‹ˆë‹¤.  
   - ì˜ˆì‹œ íŒŒì¼ëª…: `construction_terms_full.jsonl`  
   - ê° ì¤„ì´ `{ "instruction": "...", "input": "...", "output": "..." }` í˜•íƒœì—¬ì•¼ í•˜ë©°,  
     `output` í•„ë“œëŠ” ë°˜ë“œì‹œ ë¬¸ìì—´(`""`)ë¡œ ë˜í•‘ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

2. íŒŒì¸íŠœë‹ ì‹¤í–‰ ì˜ˆì‹œ
   ```bash
   cd scripts
   python run_lora_finetune.py        --data_path ../data/construction_terms_full.jsonl        --output_dir ../lora-construction-terms-output        --per_device_train_batch_size 2        --gradient_accumulation_steps 2        --max_seq_length 512        --num_train_epochs 3
   ```
   - `--data_path`: JSONL ë°ì´í„° íŒŒì¼ ê²½ë¡œ  
   - `--output_dir`: ê²°ê³¼ë¬¼ì„ ì €ì¥í•  ë””ë ‰í„°ë¦¬  
   - `--per_device_train_batch_size`: GPUë‹¹ ë°°ì¹˜ ì‚¬ì´ì¦ˆ  
   - `--gradient_accumulation_steps`: ê¸°ìš¸ê¸° ëˆ„ì  ìŠ¤í… ìˆ˜  
   - `--max_seq_length`: ì…ë ¥ ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´ (ì˜ˆ: 512)  
   - `--num_train_epochs`: í•™ìŠµ epoch ìˆ˜  

3. í•™ìŠµì´ ì™„ë£Œë˜ë©´ `lora-construction-terms-output/` í´ë”ì— ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:
   - `pytorch_model.bin`: 8-bit ì–‘ìí™”ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ + LoRA ì–´ëŒ‘í„°  
   - `adapter_config.json`: LoRA ì„¤ì • ì •ë³´  
   - `tokenizer.json`, `tokenizer_config.json`, `config.json`, `generation_config.json` ë“±

4. (ì„ íƒ) LoRA ì–´ëŒ‘í„° state_dictë§Œ ì¶”ì¶œ
   ```python
   from peft import get_peft_model_state_dict
   import torch

   peft_state_dict = get_peft_model_state_dict(model)
   torch.save(peft_state_dict, "lora_adapter_state_dict.pt")
   ```

## English Explanation
1. Place your fine-tuning data (JSONL) under the `data/` folder.  
   - Example filename: `construction_terms_full.jsonl`  
   - Each line must be of the form  
     `{ "instruction": "...", "input": "...", "output": "..." }`,  
     and `output` must always be wrapped as a string (`""`).

2. Run fine-tuning with, for example:
   ```bash
   cd scripts
   python run_lora_finetune.py        --data_path ../data/construction_terms_full.jsonl        --output_dir ../lora-construction-terms-output        --per_device_train_batch_size 2        --gradient_accumulation_steps 2        --max_seq_length 512        --num_train_epochs 3
   ```
   - `--data_path`: Path to the JSONL data file  
   - `--output_dir`: Directory to save outputs  
   - `--per_device_train_batch_size`: Batch size per GPU  
   - `--gradient_accumulation_steps`: Number of gradient accumulation steps  
   - `--max_seq_length`: Maximum input sequence length (e.g., 512)  
   - `--num_train_epochs`: Number of training epochs  

3. When training finishes, the `lora-construction-terms-output/` folder will contain:  
   - `pytorch_model.bin`: 8-bit quantized model weights + LoRA adapter  
   - `adapter_config.json`: LoRA configuration metadata  
   - `tokenizer.json`, `tokenizer_config.json`, `config.json`, `generation_config.json`, etc.

4. (Optional) Extract only the LoRA adapter state_dict:
   ```python
   from peft import get_peft_model_state_dict
   import torch

   peft_state_dict = get_peft_model_state_dict(model)
   torch.save(peft_state_dict, "lora_adapter_state_dict.pt")
   ```

---

# ğŸ”§ íŒŒì¸íŠœë‹ ì„¸ë¶€ ì˜µì…˜ ì„¤ëª… / Fine-Tuning Details

## 1. 8-bit ì–‘ìí™” / 8-bit Quantization
- **í•œê¸€**:  
  `load_in_8bit=True` ì˜µì…˜ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œí•˜ë©´,  
  A100 80GBì—ì„œë„ 11B ëª¨ë¸ì„ ë©”ëª¨ë¦¬ í•œê³„ ì—†ì´ ì˜¬ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
  FP16 í˜¼í•© ì •ë°€ë„ì™€ í˜¸í™˜ë˜ë©°, ë‚´ë¶€ì ìœ¼ë¡œ ìë™ ë³€í™˜ë©ë‹ˆë‹¤.

- **English**:  
  By setting `load_in_8bit=True`, you drastically reduce memory usage,  
  enabling you to load the 11B model even on a single A100 80GB.  
  It works seamlessly with FP16 mixed precision and is handled internally.

```python
model = AutoModelForCausalLM.from_pretrained(
    args.model_name_or_path,
    load_in_8bit=True,
    torch_dtype=torch.float16,
    device_map="auto",
)
```

---

## 2. LoRA(PEFT) ì„¤ì • / LoRA (PEFT) Configuration
- **í•œê¸€**:  
  LoRAëŠ” ì „ì²´ ëª¨ë¸ì„ í•™ìŠµí•˜ì§€ ì•Šê³ , ì¼ë¶€ ì €ì°¨ì› íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµí•˜ì—¬ ë©”ëª¨ë¦¬Â·ì‹œê°„ì„ ì ˆì•½í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.  
  `r`(rank), `alpha`, `dropout`, `target_modules` ë“±ì„ ì„¤ì •í•©ë‹ˆë‹¤.

- **English**:  
  LoRA (PEFT) fine-tuning updates only a small subset of low-rank parameters rather than the full model,  
  saving memory and training time. You specify hyperparameters like `r`, `alpha`, `dropout`, and `target_modules`.

```python
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=args.lora_rank,            # ê¸°ë³¸: 8
    lora_alpha=args.lora_alpha,  # ê¸°ë³¸: 32
    lora_dropout=args.lora_dropout,  # ê¸°ë³¸: 0.05
    target_modules=args.target_modules.split(","),  # ["q_proj","k_proj","v_proj","o_proj"]
    bias="none",
)
```

- **r (Rank)**:  
  - í•œê¸€: LoRAì˜ ì €ì°¨ì› ì„ë² ë”© ì°¨ì›. ê°’ì´ í¬ë©´ í’ˆì§ˆ ê°œì„  ê°€ëŠ¥í•˜ì§€ë§Œ, ë©”ëª¨ë¦¬/ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„ ë°œìƒ.  
  - English: Low-rank embedding dimension for LoRA. Larger values can improve performance but cost more memory/compute.

- **Î± (Alpha)**:  
  - í•œê¸€: Scaling factor, ë³´í†µ 16~32 ë²”ìœ„ê°€ ë¬´ë‚œ.  
  - English: Scaling factor; 16â€“32 is a common choice.

- **Dropout**:  
  - í•œê¸€: ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ 0.05 ì •ë„ ì„¤ì •.  
  - English: Set around 0.05 to prevent overfitting.

---

## 3. ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ / Maximum Sequence Length
- **í•œê¸€**:  
  `max_seq_length=512`ë¡œ ì„¤ì •í•˜ë©´,  
  attention ì—°ì‚° ë¹„ìš©(O(NÂ²))ì„ 1/4ë¡œ ì¤„ì—¬ ì†ë„Â·ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ í¬ê²Œ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
  ë‹¤ë§Œ, ë“œë¬¼ê²Œ 512 í† í°ì„ ë„˜ëŠ” ì˜ˆì‹œê°€ ìˆë‹¤ë©´ ë’¤ìª½ì´ ì˜ë¦´ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.

- **English**:  
  By halving from 1024â†’512, attention cost drops to (512/1024)Â² = 1/4.  
  This speeds up training and drastically reduces memory.  
  If some examples exceed 512 tokens, the tail may be truncatedâ€”check your data distribution first.

---

## 4. ë°°ì¹˜ í¬ê¸° & Gradient Accumulation / Batch Size & Gradient Accumulation
- **í•œê¸€**:  
  `per_device_train_batch_size=2, gradient_accumulation_steps=2`ë¡œ ì„¤ì •í•˜ë©´,  
  í•œ ìŠ¤í…ë‹¹ ì´ ë°°ì¹˜ í¬ê¸°ê°€ 4ê°€ ìœ ì§€ë©ë‹ˆë‹¤(2Ã—2).  
  Overheadê°€ ì¤„ì–´ë“¤ì–´ í•™ìŠµ ì†ë„ê°€ ì•½ê°„ ë¹¨ë¼ì§€ê³ , ë©”ëª¨ë¦¬ ì‚¬ìš© íš¨ìœ¨ë„ ì¢‹ì•„ì§‘ë‹ˆë‹¤.

- **English**:  
  Setting `per_device_train_batch_size=2` and `gradient_accumulation_steps=2`  
  keeps the effective batch size at 4 (2Ã—2) per step. Reduces overhead and slightly speeds up training.

```python
training_args = TrainingArguments(
    ...
    per_device_train_batch_size=2,
    gradient_accumulation_steps=2,
    ...
)
```

- **TIP**:  
  - í•œê¸€: VRAMì´ ë„‰ë„‰í•˜ë‹¤ë©´ `per_device_train_batch_size=4, gradient_accumulation_steps=1` ì²˜ëŸ¼ ì¡°ì •í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.  
  - English: If you have more VRAM, you can try `per_device_train_batch_size=4, gradient_accumulation_steps=1` to further reduce accumulation overhead.

---

## 5. ë°ì´í„° ì „ì²˜ë¦¬ ë³‘ë ¬í™” / Data Preprocessing Parallelization
- **í•œê¸€**:  
  HuggingFace `Dataset.map(..., num_proc=4)` ì˜µì…˜ì„ ì£¼ë©´,  
  CPU 4ê°œë¥¼ ë™ì‹œì— ì‚¬ìš©í•´ **í† í¬ë‚˜ì´ì¦ˆ ë° ë ˆì´ë¸” ìƒì„±**ë§Œ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.  
  í•™ìŠµ í’ˆì§ˆ(ëª¨ë¸ ì„±ëŠ¥)ì—ëŠ” ì „í˜€ ì˜í–¥ ì—†ì´ ë°ì´í„° ì¤€ë¹„ ì‹œê°„ì„ ë‹¨ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- **English**:  
  By passing `num_proc=4` to `Dataset.map(...)`, you launch 4 CPU processes to parallelize tokenization & label creation only.  
  This does not affect model performanceâ€”only speeds up data loading.

```python
tokenized_dataset = raw_dataset.map(
    preprocess_function,
    batched=True,
    num_proc=4,  # Adjust to your CPU core count
    remove_columns=raw_dataset.column_names,
)
```

---

# ğŸ“Š ê²°ê³¼ë¬¼ / Outputs

í•™ìŠµì´ ëë‚œ ë’¤ `lora-construction-terms-output/` í´ë”ì—ëŠ”:

```
lora-construction-terms-output/
â”œâ”€â”€ adapter_config.json
â”œâ”€â”€ config.json
â”œâ”€â”€ generation_config.json
â”œâ”€â”€ pytorch_model.bin       # 8bit ì–‘ìí™”ëœ ëª¨ë¸ + LoRA ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer_config.json
â””â”€â”€ training_args.bin
```

- **pytorch_model.bin**:  
  í•œê¸€: 8-bit ì–‘ìí™”ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ì™€ LoRA ì–´ëŒ‘í„° íŒŒë¼ë¯¸í„°ê°€ í•©ì³ì§„ íŒŒì¼  
  English: Combined file of 8-bit quantized base weights and LoRA adapter parameters.

- **adapter_config.json**:  
  í•œê¸€: LoRA ì„¤ì • ìƒì„¸ ì •ë³´(ëª¨ë“ˆ, rank, alpha, dropout ë“±)  
  English: Metadata about the LoRA adapter (modules, rank, alpha, dropout, etc.)

- **ê¸°íƒ€**: `tokenizer.json`, `tokenizer_config.json`, `config.json`, `generation_config.json`, `training_args.bin` ë“±  
  í•œê¸€: í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ì„¤ì • ì •ë³´  
  English: Tokenizer and model configuration files.

---

# ğŸ“œ ë¼ì´ì„ ìŠ¤ / License

- **í•œê¸€**:  
  ë³¸ í”„ë¡œì íŠ¸ëŠ” ë³„ë„ì˜ ë¼ì´ì„ ìŠ¤ ì—†ì´ â€œNoneâ€ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.  

- **English**:  
  This project currently has no license (â€œNoneâ€).  
