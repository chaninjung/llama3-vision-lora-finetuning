# Llama 3 Vision + LoRA Fine-Tuning Example

**Meta-Llama 3.2 11B Vision-Instruct** λ¨λΈμ„ 8-bit μ–‘μν™”ν• λ’¤, LoRA(PEFT)λ΅ νμΈνλ‹ν•λ” μμ  μ¤ν¬λ¦½νΈ λ¨μμ…λ‹λ‹¤.

---

## π“‹ λ©μ°¨

1. [λ°°κ²½ λ° λ©ν‘](#-λ°°κ²½-λ°-λ©ν‘)  
2. [νμΌ κµ¬μ΅°](#-νμΌ-κµ¬μ΅°)  
3. [μ‚¬μ „ μ¤€λΉ„](#-μ‚¬μ „-μ¤€λΉ„)  
4. [μ‚¬μ© λ°©λ²•](#-μ‚¬μ©-λ°©λ²•)  
5. [νμΈνλ‹ μ„Έλ¶€ μµμ… μ„¤λ…](#-νμΈνλ‹-μ„Έλ¶€-μµμ…-μ„¤λ…)  
6. [κ²°κ³Όλ¬Ό](#-κ²°κ³Όλ¬Ό)  
7. [λΌμ΄μ„ μ¤](#-λΌμ΄μ„ μ¤)  
8. [μ°Έκ³  μλ£](#-μ°Έκ³ -μλ£)

---

## π“ λ°°κ²½ λ° λ©ν‘

- **λ©ν‘**  
  - A100 80GB GPU ν• μ¥μ—μ„ Meta-Llama/Llama-3.2-11B-Vision-Instruct λ¨λΈμ„ 8-bit μ–‘μν™”λ΅ λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ„ μ¤„μ΄κ³ ,  
    LoRA(PEFT)λ΅ κ·Ήν μΌλ¶€ νλΌλ―Έν„°λ§ ν•™μµν•μ—¬ νΉμ • λ„λ©”μΈ(μ: κ±΄μ„¤ μ©μ–΄) μμ μ— λ§μ¶° νμΈνλ‹ν•λ” λ°©λ²•μ„ μ‹μ—°ν•©λ‹λ‹¤.  
  - λ΅μ»¬(Colab, μ‚¬λ‚΄ μ„λ²„ λ“±) ν™κ²½μ—μ„  
    1. FP16 νΌν•© μ •λ°€λ„,  
    2. GPU λ©”λ¨λ¦¬ μ μ•½(8-bit μ–‘μν™”),  
    3. LoRA νμΈνλ‹ μ‚¬μ©λ²•μ„ ν•λμ— μ• μ μλ„λ΅ μμ  μ¤ν¬λ¦½νΈμ™€ μ„¤λ…μ„ μ κ³µν•©λ‹λ‹¤.

---

## π“‚ νμΌ κµ¬μ΅°
```
llama3-vision-lora-finetuning/
β”β”€β”€ .gitignore
β”β”€β”€ LICENSE
β”β”€β”€ README.md
β”β”€β”€ requirements.txt
β”β”€β”€ scripts/
β”‚   β””β”€β”€ run_lora_finetune.py
β”β”€β”€ data/
β”‚   β””β”€β”€ placeholder.txt
β””β”€β”€ README_IMAGES/
```

- **`.gitignore`**  
  λ¶ν•„μ”ν• μΊμ‹ νμΌμ΄λ‚ λ€μ©λ‰ μ²΄ν¬ν¬μΈνΈ ν΄λ”λ¥Ό μ μ™Έν•©λ‹λ‹¤.

- **`LICENSE`**  
  MIT License (μ„ νƒν• λΌμ΄μ„ μ¤) λλ” ν•„μ” μ‹ λ‹¤λ¥Έ μ¤ν”μ†μ¤ λΌμ΄μ„ μ¤λ¥Ό λ…μ‹ν•©λ‹λ‹¤.

- **`README.md`**  
  μ΄ νμΌ μμ²΄μ…λ‹λ‹¤. ν”„λ΅μ νΈ κ°μ”, μ„¤μΉ λ° μ‚¬μ© λ°©λ²•, μµμ… μ„¤λ… λ“±μ„ ν•λμ— λ³΄μ—¬μ¤λ‹λ‹¤.

- **`requirements.txt`**  
  νμΈνλ‹μ— ν•„μ”ν• Python ν¨ν‚¤μ§€μ™€ μµμ† λ²„μ „μ„ λ‚μ—΄ν•΄ λ‘΅λ‹λ‹¤.

- **`scripts/run_lora_finetune.py`**  
  LoRA κΈ°λ° νμΈνλ‹ μ¤ν¬λ¦½νΈ μ „μ²΄ μ½”λ“μ…λ‹λ‹¤.  
  - λ°μ΄ν„° λ΅λ“ β†’ μ „μ²λ¦¬ β†’ λ¨λΈ λ΅λ“(8-bit μ–‘μν™”) β†’ LoRA μ„¤μ • β†’ Trainerλ΅ ν•™μµ β†’ LoRA μ–΄λ‘ν„° μ €μ¥ μμΌλ΅ κµ¬μ„±λμ–΄ μμµλ‹λ‹¤.

- **`data/placeholder.txt`**  
  μ‹¤μ  JSONL λ°μ΄ν„°λ” ν¬ν•¨ν•μ§€ μ•κ³ , β€μ—¬κΈ°μ— νμΈνλ‹ λ°μ΄ν„°(construction_terms_full.jsonl λ“±)λ¥Ό λ„£μΌμ„Έμ”β€λΌλ” μ•λ‚΄λ¬Έλ§ λ‚¨κ²¨λ‘΅λ‹λ‹¤.

- **`README_IMAGES/`**  
  (μ„ νƒ) ν”„λ΅μ νΈ κµ¬μ΅°λ‚ νμ΄ν”„λΌμΈ λ‹¤μ΄μ–΄κ·Έλ¨μ„ λ³΄μ—¬μ£Όλ” μ΄λ―Έμ§€ νμΌμ„ λ‘λ” ν΄λ”μ…λ‹λ‹¤.

---

## β™οΈ μ‚¬μ „ μ¤€λΉ„

1. **Python 3.9+**
2. **A100 80GB GPU** (λλ” 8-bit μ–‘μν™”/LoRAλ¥Ό μ‚¬μ©ν•  μ μλ” μ¶©λ¶„ν• VRAMμ„ κ°–μ¶ GPU)
3. **λ΅μ»¬ μ €μ¥μ† ν΄λ΅  λ° ν¨ν‚¤μ§€ μ„¤μΉ**  
   ```bash
   git clone https://github.com/YourUser/llama3-vision-lora-finetuning.git
   cd llama3-vision-lora-finetuning
   python3 -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```
4. **HuggingFace λ΅κ·ΈμΈ (ν•„μ” μ‹)**  
   ```bash
   huggingface-cli login
   ```
   ν—κΉ…νμ΄μ¤ ν—λΈμ—μ„ λ¨λΈμ„ λ‹¤μ΄λ°›κΈ° μ„ν•΄ μ‚¬μ „μ— ν† ν°μ„ μ„¤μ •ν•΄ λ‘λ©΄ μ›ν™ν•κ² μ§„ν–‰λ©λ‹λ‹¤.

---

## π€ μ‚¬μ© λ°©λ²•

1. **λ°μ΄ν„° μ¤€λΉ„**  
   - `data/` ν΄λ”μ— JSONL ν•νƒμ νμΈνλ‹ λ°μ΄ν„°λ¥Ό λ„£μµλ‹λ‹¤.  
   - νμΌ μ΄λ¦„ μμ‹: `construction_terms_full.jsonl`  
   - κ° μ¤„μ΄ `{ "instruction": "...", "input": "...", "output": "..." }` ν•νƒμ΄μ–΄μ•Ό ν•©λ‹λ‹¤.  
   - **μ£Όμ**: `output` ν•„λ“λ” λ°λ“μ‹ λ¬Έμμ—΄λ΅ λν•‘ν•΄μ•Ό ν•λ©°, μ«μλ‚ nullμ΄ μ„μ΄μ§€ μ•λ„λ΅ ν•©λ‹λ‹¤.

2. **νμΈνλ‹ μ‹¤ν–‰**  
   ν„°λ―Έλ„μ—μ„ λ‹¤μ μμ‹ λ…λ Ήμ–΄λ¥Ό μ°Έκ³ ν•΄ μ μ ν μµμ…μ„ λ°”κΏ” μ‹¤ν–‰ν•μ„Έμ”:

   ```bash
   cd scripts
   python run_lora_finetune.py        --data_path ../data/construction_terms_full.jsonl        --output_dir ../lora-construction-terms-output        --per_device_train_batch_size 2        --gradient_accumulation_steps 2        --max_seq_length 512        --num_train_epochs 3
   ```

   - `--data_path`: JSONL λ°μ΄ν„° νμΌ κ²½λ΅  
   - `--output_dir`: κ²°κ³Όλ¬Όμ„ μ €μ¥ν•  λ””λ ‰ν„°λ¦¬  
   - `--per_device_train_batch_size`: GPUλ‹Ή λ°°μΉ μ‚¬μ΄μ¦  
   - `--gradient_accumulation_steps`: κΈ°μΈκΈ° λ„μ  μ¤ν… μ  
   - `--max_seq_length`: μ…λ ¥ μ‹ν€€μ¤ μµλ€ ν† ν° κΈΈμ΄ (μ: 512)  
   - `--num_train_epochs`: ν•™μµ epoch μ

3. **ν•™μµ κ²°κ³Ό ν™•μΈ**  
   ν•™μµμ΄ μ™„λ£λλ©΄ `lora-construction-terms-output/` ν΄λ”μ—  
   - `pytorch_model.bin` (8-bit μ–‘μν™”λ λ¨λΈ + LoRA μ–΄λ‘ν„° κ°€μ¤‘μΉ)  
   - `adapter_config.json`, `config.json`, `tokenizer.json` λ“± νμΌμ΄ μ €μ¥λ©λ‹λ‹¤.

   ν•„μ” μ‹ μ•„λ μ½”λ“λ΅ LoRA μ–΄λ‘ν„° `state_dict`λ§ μ¶”μ¶ν•  μ μμµλ‹λ‹¤:

   ```python
   from peft import get_peft_model_state_dict
   import torch

   peft_state_dict = get_peft_model_state_dict(model)
   torch.save(peft_state_dict, "lora_adapter_state_dict.pt")
   ```

---

## π”§ νμΈνλ‹ μ„Έλ¶€ μµμ… μ„¤λ…

1. **8-bit μ–‘μν™” (`load_in_8bit=True`)**  
   - A100 80GBμ—μ„λ„ 11B λ¨λΈμ„ μ¬λ ¤λ†“μ„ μ μλ„λ΅ λ©”λ¨λ¦¬λ¥Ό ν¬κ² μ¤„μ—¬μ¤λ‹λ‹¤.  
   - FP16 νΌν•© μ •λ°€λ„μ™€ νΈν™λλ©°, λ‚΄λ¶€μ μΌλ΅ μλ™ λ³€ν™λ©λ‹λ‹¤.  

   ```python
   model = AutoModelForCausalLM.from_pretrained(
       args.model_name_or_path,
       load_in_8bit=True,
       torch_dtype=torch.float16,
       device_map="auto",
   )
   ```

2. **LoRA(PEFT) μ„¤μ •**  
   ```python
   lora_config = LoraConfig(
       task_type=TaskType.CAUSAL_LM,
       inference_mode=False,
       r=args.lora_rank,            # κΈ°λ³Έκ°’: 8
       lora_alpha=args.lora_alpha,  # κΈ°λ³Έκ°’: 32
       lora_dropout=args.lora_dropout,  # κΈ°λ³Έκ°’: 0.05
       target_modules=args.target_modules.split(","),  # ["q_proj","k_proj","v_proj","o_proj"]
       bias="none",
   )
   ```

   - **r (rank)**: LoRA μ €μ°¨μ› μ„λ² λ”© μ°¨μ›. κ°’μ΄ ν¬λ©΄ μ„±λ¥ ν–¥μƒμ΄ κ°€λ¥ν•μ§€λ§, λ©”λ¨λ¦¬/μ†λ„ νΈλ μ΄λ“μ¤ν”„κ°€ μμµλ‹λ‹¤.  
   - **Ξ± (alpha)**: Scaling factor. λ³΄ν†µ 16~32 μ‚¬μ΄κ°€ λ¬΄λ‚ν•©λ‹λ‹¤.  
   - **dropout**: μ¤λ²„ν”Όν… λ°©μ§€λ¥Ό μ„ν•΄ 0.05 μ •λ„ μ„¤μ •ν•©λ‹λ‹¤.  

3. **μµλ€ μ‹ν€€μ¤ κΈΈμ΄ (`max_seq_length=512`)**  
   - κΈ°λ³Έ λ¨λΈ(1024) λ€λΉ„ μ λ°μΌλ΅ μ¤„μ—¬μ„ ν•™μµ μ†λ„λ¥Ό μ•½ 2λ°°, λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ„ 1/4 μ΄μƒ μ κ°ν•  μ μμµλ‹λ‹¤.  
   - λ°μ΄ν„° μμ‹ μ¤‘ 600~800 ν† ν°μ„ μ‚¬μ©ν•λ” ν•­λ©μ΄ μλ‹¤λ©΄, λ’·λ¶€λ¶„μ΄ μλ¦΄ μ μμΌλ‹ μ‚¬μ „μ— λ°μ΄ν„° λ¶„ν¬λ¥Ό ν™•μΈν•΄μ£Όμ„Έμ”.

4. **λ°°μΉ ν¬κΈ° & Gradient Accumulation**  
   - `per_device_train_batch_size=2`, `gradient_accumulation_steps=2` β†’  
     ν• μ¤ν…λ‹Ή μ΄ λ°°μΉ μ‚¬μ΄μ¦ 4λ¥Ό μ μ§€ν•λ©΄μ„,  
     λ‚΄λ¶€ Overhead(GPU λ©”λ¨λ¦¬ μ΄ν„°λ μ΄μ…)κ°€ μ΅°κΈ μ¤„μ–΄λ“­λ‹λ‹¤.

   VRAM μ—¬μ κ°€ μ¶©λ¶„ν•λ‹¤λ©΄:  
   - `per_device_train_batch_size=4`, `gradient_accumulation_steps=1`  
   - `per_device_train_batch_size=8`, `gradient_accumulation_steps=1`  
   λ“±μΌλ΅ μ΅°μ •ν•  μ μμµλ‹λ‹¤.

5. **λ°μ΄ν„° μ „μ²λ¦¬ λ³‘λ ¬ν™” (`num_proc=4`)**  
   ```python
   tokenized_dataset = raw_dataset.map(
       preprocess_function,
       batched=True,
       num_proc=4,  # CPU μ½”μ–΄ 4κ°λ΅ λ³‘λ ¬ μ²λ¦¬
       remove_columns=raw_dataset.column_names,
   )
   ```  
   - CPU 4κ°(λλ” μ‚¬μ© κ°€λ¥ν• μ½”μ–΄ μ)λ΅ ν† ν¬λ‚μ΄μ € ν•¨μλ¥Ό λ³‘λ ¬ νΈμ¶ν•μ—¬,  
     λ€λ‰μ JSONL λ°μ΄ν„°λ¥Ό λΉ λ¥΄κ² μ „μ²λ¦¬ν•  μ μμµλ‹λ‹¤.  
   - ν•™μµ ν’μ§(λ¨λΈ μ„±λ¥)μ—λ” μ „ν€ μν–¥μ„ μ£Όμ§€ μ•κ³ , λ°μ΄ν„° μ¤€λΉ„ μ‹κ°„λ§ λ‹¨μ¶•λ©λ‹λ‹¤.

---

## π“ κ²°κ³Όλ¬Ό

ν•™μµμ΄ λλ‚ ν›„ `lora-construction-terms-output/` μ•μ—λ” λ‹¤μκ³Ό κ°™μ€ μ£Όμ” νμΌμ΄ μƒμ„±λ©λ‹λ‹¤:

```
lora-construction-terms-output/
β”β”€β”€ adapter_config.json
β”β”€β”€ config.json
β”β”€β”€ generation_config.json
β”β”€β”€ pytorch_model.bin       # 8bit μ–‘μν™” + LoRA μ–΄λ‘ν„° κ°€μ¤‘μΉ ν¬ν•¨
β”β”€β”€ tokenizer.json
β”β”€β”€ tokenizer_config.json
β””β”€β”€ training_args.bin
```

- `pytorch_model.bin`: μ–‘μν™”λ λ¨λΈ κ°€μ¤‘μΉμ™€ LoRA μ–΄λ‘ν„° νλΌλ―Έν„°κ°€ ν•©μ³μ Έ μλ” νμΌμ…λ‹λ‹¤.  
- `adapter_config.json`: LoRA μ–΄λ‘ν„°μ— κ΄€ν• λ©”νƒ€μ •λ³΄(λ¨λ“, rank, alpha, dropout λ“±).  
- ν•„μ”ν•λ‹¤λ©΄, μμ LoRA μ–΄λ‘ν„° `state_dict`λ§ μ¶”μ¶ν•΄μ„ λ³„λ„ μ €μ¥ν•  μ μμµλ‹λ‹¤:

  ```python
  from peft import get_peft_model_state_dict
  import torch

  peft_state_dict = get_peft_model_state_dict(model)
  torch.save(peft_state_dict, "lora_adapter_state_dict.pt")
  ```

---

## π“ λΌμ΄μ„ μ¤

μ΄ μ €μ¥μ†λ” λΌμ΄μ„ μ¤ μ—†μ (None) μΌλ΅ μ„¤μ •λμ–΄ μμµλ‹λ‹¤.

---

## π“– μ°Έκ³  μλ£

- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers/)  
- [PEFT LoRA Documentation](https://github.com/huggingface/peft)  
- [Meta-Llama GitHub](https://github.com/facebookresearch/llama)  
